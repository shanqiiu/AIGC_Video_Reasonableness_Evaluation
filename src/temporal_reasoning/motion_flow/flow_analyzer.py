# -*- coding: utf-8 -*-
"""
å…‰æµåˆ†æå™?
"""

import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict
from tqdm import tqdm
import torch
import cv2

from .raft_wrapper import RAFTWrapper
from .motion_smoothness import (
    compute_motion_smoothness,
    detect_motion_discontinuities,
    compute_flow_statistics
)
from ..core.config import RAFTConfig


class MotionFlowAnalyzer:
    """å…‰æµåˆ†æå™?"""
    
    def __init__(self, config: RAFTConfig):
        """
        åˆå§‹åŒ–å…‰æµåˆ†æå™¨
        
        Args:
            config: RAFTConfigé…ç½®å¯¹è±¡
        """
        self.config = config
        self.raft_model = None
        # æ ¹æ®use_gpué…ç½®æ­£ç¡®è®¾ç½®è®¾å¤‡å­—ç¬¦ä¸?
        if config.use_gpu and torch.cuda.is_available():
            self.device = "cuda:0"
        else:
            self.device = "cpu"
    
    def initialize(self):
        """åˆå§‹åŒ–RAFTæ¨¡å‹"""
        print("æ­£åœ¨åˆå§‹åŒ–å…‰æµåˆ†æå™¨...")
        try:
            self.raft_model = RAFTWrapper(
                model_path=self.config.model_path,
                model_type=self.config.model_type,
                device=self.device
            )
            print("å…‰æµåˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼?")
        except Exception as e:
            print(f"è­¦å‘Š: å…‰æµåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨ç®€åŒ–å®ç?")
            # åˆå§‹åŒ–å¤±è´¥æ—¶ï¼Œraft_modelä¿æŒä¸ºNone
            # analyze()æ–¹æ³•ä¼šæ£€æŸ¥å¹¶æŠ›å‡ºå¼‚å¸¸
    
    def analyze(
        self,
        video_frames: List[np.ndarray],
        fps: float = 30.0
    ) -> Tuple[float, List[Dict]]:
        """
        åˆ†æè§†é¢‘è¿åŠ¨å¹³æ»‘åº?
        
        Args:
            video_frames: è§†é¢‘å¸§åºåˆ—ï¼Œæ¯å¸§ä¸ºRGBå›¾åƒ (H, W, 3)
            fps: è§†é¢‘å¸§ç‡ï¼Œç”¨äºè®¡ç®—æ—¶é—´æˆ³
        
        Returns:
            (motion_score, anomalies): 
            - motion_score: è¿åŠ¨åˆç†æ€§å¾—åˆ? (0-1)
            - anomalies: è¿åŠ¨å¼‚å¸¸åˆ—è¡¨
        """
        if len(video_frames) < 2:
            return 1.0, []
        
        if self.raft_model is None:
            raise RuntimeError(
                "RAFTæ¨¡å‹æœªåˆå§‹åŒ–\n"
                "è¯·å…ˆè°ƒç”¨ initialize() æ–¹æ³•åˆå§‹åŒ–æ¨¡å?"
            )
        
        # 1. è®¡ç®—å…‰æµåºåˆ—
        print("æ­£åœ¨è®¡ç®—å…‰æµ...")
        optical_flows = []
        for i in tqdm(range(len(video_frames) - 1), desc="è®¡ç®—å…‰æµ"):
            try:
                u, v = self.raft_model.compute_flow(video_frames[i], video_frames[i+1])
                optical_flows.append((u, v))
            except Exception as e:
                raise RuntimeError(
                    f"ç¬¬{i}å¸§å…‰æµè®¡ç®—å¤±è´?: {e}\n"
                    f"è¯·æ£€æŸ¥RAFTæ¨¡å‹æ˜¯å¦æ­£ç¡®åˆå§‹åŒ?"
                )
        
        if not optical_flows:
            return 1.0, []
        
        # 2. è®¡ç®—è¿åŠ¨å¹³æ»‘åº?
        print("æ­£åœ¨åˆ†æè¿åŠ¨å¹³æ»‘åº?...")
        motion_smoothness = compute_motion_smoothness(optical_flows)
        
        if not motion_smoothness:
            return 1.0, []
        
        # 3. æ£€æµ‹è¿åŠ¨çªå?
        print("æ­£åœ¨æ£€æµ‹è¿åŠ¨çªå?...")
        # ä»é…ç½®ä¸­è·å–é˜ˆå€?
        threshold = getattr(self.config, 'motion_discontinuity_threshold', 0.3)
        motion_anomalies = detect_motion_discontinuities(
            optical_flows,
            threshold=threshold,
            fps=fps
        )
        
        # 4. è®¡ç®—å¾—åˆ†
        base_score = float(np.mean(motion_smoothness))
        
        # å¼‚å¸¸æƒ©ç½šï¼šæ¯ä¸ªå¼‚å¸¸æ‰£åˆ?
        anomaly_penalty = min(0.5, len(motion_anomalies) * 0.1)
        final_score = max(0.0, base_score * (1.0 - anomaly_penalty))
        
        # 5. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        flow_stats = compute_flow_statistics(optical_flows)
        
        print(f"è¿åŠ¨åˆç†æ€§å¾—åˆ?: {final_score:.3f}")
        print(f"æ£€æµ‹åˆ° {len(motion_anomalies)} ä¸ªè¿åŠ¨å¼‚å¸?")

        if getattr(self.config, "enable_visualization", False):
            self._save_visualizations(video_frames, optical_flows, motion_anomalies)
        
        return final_score, motion_anomalies

    @staticmethod
    def _flow_to_color(u: np.ndarray, v: np.ndarray) -> np.ndarray:
        magnitude, angle = cv2.cartToPolar(u, v, angleInDegrees=False)
        hsv = np.zeros((u.shape[0], u.shape[1], 3), dtype=np.uint8)
        hsv[..., 0] = (angle * 180 / np.pi / 2).astype(np.uint8)
        hsv[..., 1] = 255
        hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

    def _select_frames_for_visualization(
        self,
        num_flows: int,
        anomalies: List[Dict],
    ) -> List[int]:
        candidate_frames = sorted(
            {a.get("frame_id") for a in anomalies if isinstance(a, dict) and "frame_id" in a}
        )
        candidate_frames = [idx for idx in candidate_frames if isinstance(idx, int)]
        max_frames = max(0, getattr(self.config, "visualization_max_frames", 0))
        if not candidate_frames:
            if max_frames <= 0 or num_flows == 0:
                return []
            stride = max(1, num_flows // max_frames) if max_frames > 0 else 1
            candidate_frames = list(range(0, num_flows, stride))
        if max_frames > 0:
            candidate_frames = candidate_frames[:max_frames]
        return candidate_frames

    def _save_visualizations(
        self,
        video_frames: List[np.ndarray],
        optical_flows: List[Tuple[np.ndarray, np.ndarray]],
        motion_anomalies: List[Dict],
    ) -> None:
        frame_indices = self._select_frames_for_visualization(len(optical_flows), motion_anomalies)
        if not frame_indices:
            return

        if self.config.visualization_output_dir:
            output_dir = Path(self.config.visualization_output_dir).expanduser().resolve()
        else:
            output_dir = Path(__file__).resolve().parents[3] / "outputs" / "motion_flow"
        output_dir.mkdir(parents=True, exist_ok=True)

        for idx in frame_indices:
            if idx >= len(optical_flows) or idx >= len(video_frames):
                continue
            u, v = optical_flows[idx]
            flow_color = self._flow_to_color(u, v)

            frame_rgb = video_frames[idx]
            if frame_rgb.dtype != np.uint8:
                frame_rgb = np.clip(frame_rgb, 0, 255).astype(np.uint8)
            if frame_rgb.shape[-1] == 3:
                frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)
            else:
                frame_bgr = frame_rgb

            blended = cv2.addWeighted(flow_color, 0.7, frame_bgr, 0.3, 0.0)
            save_path = output_dir / f"flow_{idx:04d}.png"
            cv2.imwrite(str(save_path), blended)